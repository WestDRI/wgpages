+++
title = "Managing large hierarchical datasets with PyTables"
slug = "pytables"
katex = true
+++

# https://wgpages.netlify.app/pytables

{{<cor>}}Thursday, April 27, 2023{{</cor>}}\
{{<cgr>}}2:30pm - 4pm{{</cgr>}}

## Abstract

{{<a "https://www.pytables.org" "PyTables">}} is a free and open-source Python library for managing large
hierarchical datasets. It is built on top of numpy and the HDF5 scientific dataset library, and it focuses
both on performance and interactive analysis of very large datasets. For large data streams (think
multi-dimensional arrays or billions of records) it outperforms databases in terms of speed, memory usage and
I/O bandwidth, although it is not a replacement to traditional relational databases as PyTables does not
support broad relationships between dataset variables. PyTables can be even used to organize a workflow with
many (thousands to millions) of small files, as you can create a PyTables database of nodes that can be used
like regular opened files in Python. This lets you store a large number of arbitrary files in a PyTables
database with on-the-fly compression, making it very efficient for handling huge amounts of data.

## Requirements to run PyTables in Python

1. a recent version of Python
1. the HDF5 (C flavor) library from http://www.hdfgroup.org
1. the NumPy package

## Installation

```sh
laptop
brew install python   # installs into /opt/homebrew/bin/python3
export PATH="$(brew --prefix)/opt/python@3/libexec/bin:$PATH"   # add this to your ~/.bashrc
                                                                # so that it finds pip, python
brew install hdf5
brew install virtualenv

pip install --upgrade pip
pip install pip_search
pip_search tables   # first entry, version 3.8.0, "Hierarchical datasets for Python"

cd
virtualenv pytables-env
source ~/pytables-env/bin/activate
env HDF5_DIR=/opt/homebrew/opt/hdf5 pip install tables
pip install netCDF4   # if you want to work with NetCDF data too
pip install Pillow    # if you want to work with images
...
deactivate
```








## Classical intro

**Goal**: easily manipulate data tables and array objects in a hierarchical structure (HDF5 file).

```sh
cd ~/tmp/pytables
source ~/pytables-env/bin/activate
python
```

Imagine we are compiling a catalogue of stars in which we record each star's properties:

```py
import tables as tt
import numpy as np

class Star(tt.IsDescription):
    name     = tt.StringCol(16)   # 16-character string
    ra       = tt.Float64Col()    # Right Ascension as double-precision float
    dec      = tt.Float64Col()    # Declination as double-precision float
    white    = tt.Float32Col()    # white magnitude as single-precision float
    red      = tt.Float32Col()    # red magnitude as single-precision float
    blue     = tt.Float32Col()    # blue magnitude as single-precision float
    parallax = tt.Float32Col()    # parallax as single-precision float
    proper   = tt.Float32Col()    # proper motion as single-precision float

h5file = tt.open_file("catalogue.h5", mode="w", title="Stellar catalogue")
g1 = h5file.create_group("/", 'data', 'Observational data')        # new group under root node; title and description
table = h5file.create_table(g1, 'readout', Star, "Sample stars")   # table stored as a node named "readout"
                                                                   # `Star` class defines its columns
print(h5file)   # show the object tree
h5file          # show more info

import random
from math import asin, degrees
star = table.row   # get a pointer to the Row instance of this table instance
for i in range(10):
    star['name'] = f'Star: {i:6d}'
    star['ra'] = 360 * random.random()
    star['dec'] = degrees(asin(2*random.random()-1))
    star['white'] = random.gauss()
    star['red'] = random.gauss()
    star['blue'] = random.gauss()
    star['parallax'] = 1.e-4 * random.random()
    star['proper'] = random.random()
    star.append()   # insert this new star into the catalogue

table.flush()   # flush the table's I/O buffer: write to disk, free memory resources
h5file.close()
```

```py
import tables as tt
import numpy as np
h5file = tt.open_file("catalogue.h5", mode="a")   # append mode

table = h5file.root.data.readout
table.cols               # 8 columns from Star class
table.shape              # 10 rows
table.col('name')        # data in the "name" column
table.col('parallax')    # data in the "parallax" column
table.cols.parallax[:]   # same
for row in table.iterrows():
    print(row[:])   # print each row

abc
pressure = [row['pressure'] for row in table.iterrows() if row['TDCcount'] > 3 and 20 <= row['pressure'] < 50]
pressure   # [25.0, 36.0, 49.0]

condition = 'TDCcount > 3'
for row in table.where(condition):
	print(row[:])

condition = '(TDCcount > 3) & (20 <= pressure) & (pressure < 50)'
for row in table.where(condition):
	print(row[:])

names = [row['name'] for row in table.where("(TDCcount > 3) & (20 <= pressure) & (pressure < 50)")]
names   # [b'Star:      5', b'Star:      6', b'Star:      7']

gcolumns = h5file.create_group(h5file.root, "columns", "Pressure and Name")   # new group
h5file.create_array(gcolumns, 'pressure', np.array(pressure), "Pressure column selection")
h5file.create_array(gcolumns, 'name', names, "Name column selection")

h5file.close()
```

```sh
h5dump tutorial1.h5     # examine this file (full content)
h5ls -rd tutorial1.h5   # shorter description
```

```py
import tables as tt
import numpy as np
h5file = tt.open_file("tutorial1.h5", mode="a")   # append mode

for node in h5file:   # root, 2 groups, 2 arrays, 1 table
    print(node)

for group in h5file.walk_groups():   # two groups
    print(group)

for array in h5file.walk_nodes("/", "Array"):
    print(array)

for leaf in h5file.root.detector._f_walknodes('Leaf'):   # 1 leaf = 'Readout example' table
    print(leaf)

table = h5file.root.detector.readout
table.shape
table.attrs.gath_date = "Wed, 06/12/2003 18:33"
table.attrs.temperature = 18.4
table.attrs.temp_scale = "Celsius"
print(table.attrs)
del table.attrs.gath_date   # delete an attribute

table = h5file.root.detector.readout
particle = table.row
for i in range(10, 15):   # add data to the existing table
    particle['name']  = f'Star: {i:6d}'
    particle['TDCcount'] = i % 256
    particle['ADCcount'] = (i * 256) % (1 << 16)
    particle['grid_i'] = i
    particle['grid_j'] = 10 - i
    particle['pressure'] = float(i*i)
    particle['energy'] = float(particle['pressure'] ** 4)
    particle['idnumber'] = i * (2 ** 34)
    particle.append()

table.shape
table.flush()

h5file.root.columns.pressure[:]
pres = h5file.root.columns.pressure
pres
pres[:]
pres[1:3] = [2.1, 3.5]   # overwrite the last two values
pres.flush()
```

more in this tutorial https://www.pytables.org/usersguide/tutorials.html










## Writing larger arrays

```sh
cd ~/tmp/pytables
source ~/pytables-env/bin/activate
python
```
```py
import tables as tt
import numpy as np

import netCDF4 as nc
ds = nc.Dataset("/Users/razoumov/Documents/training/paraviewWorkshop/data/mandelbulb300.nc")
print(ds)
print(ds["stability"].shape)

h5file = tt.open_file("uncompressed.h5", mode="w", title="Mandelbulb at 300^3 resolution")
g1 = h5file.create_group("/", 'g1', 'Fractal data')   # new group under root node; title and description
h5file.create_array(g1, 'stability', ds["stability"][:,:,:], "stability variable")
h5file.close()
```

The resulting file is much bigger (103M = 300^3 in single precision) than the original (1.0M). Let's use
compression:

```py
compress = tt.Filters(complib='zlib', complevel=5)
h5file = tt.open_file("compressed.h5", mode="w", title="Mandelbulb at 300^3 resolution", filters=compress)
g1 = h5file.create_group("/", 'g1', 'Fractal data')   # new group under root node; title and description
h5file.create_carray('/g1', 'stability', obj=ds["stability"][:,:,:])   # use chunked arrays, write into g1
h5file.close()
```

The compressed file is 1.4M.

Let's append a second array to the same group:

```py
h5file = tt.open_file("compressed.h5", mode="a", filters=compress)   # append mode
a = np.random.randn(100,100)   # 100^2 array from a normal (Gaussian with x0=0, sigma=1) distribution
h5file.create_carray('/g1', 'a', obj=a)   # use chunked arrays, write into g1
h5file.close()
```

Let's read this file:

```py
import tables as tt
f1 = tt.open_file("compressed.h5", mode="r")   # read mode
for node in f1:   # root, 1 group g1, 2 arrays inside g1, both compressed
    print(node)

for array in f1.walk_nodes("/", "Array"):
    print('---')
    print(array)
    print(array.shape)
    print(array[:])

f1.root.g1.a[:]                  # entire array

f1.root.g1.stability[:]          # entire array
f1.root.g1.stability[:3,:3,:3]   # 3x3x3 starting corner block
f1.root.g1.stability.shape
f1.root.g1.stability.filters
f1.root.g1.stability.title
f1.root.g1.stability.dtype
f1.root.g1.stability.size_in_memory   # uncompressed in memory
f1.close()
```

Let's remove the array `a` from the file:

```py
import tables as tt
f1 = tt.open_file("compressed.h5", mode="a")   # append mode
f1.root.g1.a.remove()
f1.close()
```

Let's create a soft link inside the file:

```py
import tables as tt
f1 = tt.open_file("compressed.h5", mode="a")   # append mode
f1.create_soft_link("/", "stab", "/g1/stability")   # link in root called "stab" pointing to "/g1/stability"
f1.root.stab.shape
f1.root.stab[:]
f1.close()
```

```sh
ptdump compressed.h5   # very useful command: dumps all objects from the file
```

Let's view this array in ParaView 5.11. Create a file `compressed.xmf` with the following:

<?xml version="1.0" ?>
<!DOCTYPE Xdmf SYSTEM "Xdmf.dtd" []>
<Xdmf Version="2.0">
 <Domain>
   <Grid Name="mesh" GridType="Uniform">
     <Topology TopologyType="3DCoRectMesh" NumberOfElements="300 300 300"/>
     <Geometry GeometryType="Origin_DxDyDz">
       <DataItem Dimensions="3" NumberType="Float" Precision="4" Format="XML">
          1 2 3
       </DataItem>
       <DataItem Dimensions="3" NumberType="Float" Precision="4" Format="XML">
        0.1 0.1 0.1
       </DataItem>
     </Geometry>
     <Attribute Name="stability" AttributeType="Scalar" Center="Node">
       <DataItem Dimensions="300 300 300" NumberType="Int" Precision="4" Format="HDF">
        compressed.h5:/g1/stability
       </DataItem>
     </Attribute>
   </Grid>
 </Domain>
</Xdmf>

Open this file in ParaView using the XDMF Reader and visualize it as usual.








## Simulating a filesystem with PyTables

- details at https://www.pytables.org/usersguide/filenode.html
- `filenode` module creates a PyTables database of nodes which can be used like regular opened files in Python

You can use file nodes and PyTables groups to mimic a filesystem with files and directories, all inside a
single HDF5 file. This lets you use HDF5 as a portable compressed backup, across different computer platforms
and architectures.

```py
import tables as tt
from tables.nodes import filenode

f1 = tt.open_file('storage.h5', 'w')                         # create a new HDF5 file
fnode = filenode.new_node(f1, where='/', name='text_file')   # create a new node file inside this file
print(f1.get_node_attr('/text_file', 'NODE_TYPE'))           # it looks like a file inside our HDF5 file

fnode.write("This is a sample line.\n".encode('utf8'))
fnode.write("Write a second line.\n".encode('ascii'))
fnode.write(b"Write a third line.\n")

fnode.seek(0)   # rewind to the beginning of the file
for line in fnode:
    print(line)

fnode.close()
print(fnode.closed)
f1.close()
```

```sh
ls -l storage.h5    # 68K
ptdump storage.h5   # root /, file /text_file
```

Let's open it again to read data:

```py
import tables as tt
from tables.nodes import filenode
f1 = tt.open_file('storage.h5', 'r')
node = f1.root.text_file
fnode = filenode.open_node(node, 'r')
for line in fnode:
    print(line)

fnode.close()
f1.close()
```

Let's add some data:

```py
import tables as tt
from tables.nodes import filenode
f1 = tt.open_file('storage.h5', 'a')                  # append mode
fnode = filenode.open_node(f1.root.text_file, 'a+')   # read and append mode
fnode.write(b"Write a fourth line.\n")
fnode.seek(0)   # rewind to the beginning of the file
for line in fnode:
    print(line)

fnode.close()
f1.close()
```

```sh
ls -l tuscany.avif               # 2874x2154 file
for num in $(seq -w 00 19); do   # crop it into twenty 300x300 random images
    echo $num
    x=$(echo "scale=8; $RANDOM / 32767 * (2874-300)" | bc)   # $RANDOM goes from 0 to 32767
    x=$(echo $x | awk '{print int($1+0.5)}')
    y=$(echo "scale=8; $RANDOM / 32767 * (2154-300)" | bc)
    y=$(echo $y | awk '{print int($1+0.5)}')
	convert tuscany.avif -crop 300x300+$x+$y small$num.png
done
```

Normally, in Pillow you read, display, write individual images with:

```py
from PIL import Image
image = Image.open('small00.png')
image.show()
image.save('copy00.png')
```

Let's try to copy all our images into our HDF5 file `storage.h5`:

```py
import tables as tt
from tables.nodes import filenode
from PIL import Image
from glob import glob

f1 = tt.open_file('storage.h5', 'a')
tuscany = f1.create_group("/", 'tuscany')

for input in glob("small*.png"):
    print('copying', input)
    image = Image.open(input)
    fnode = filenode.new_node(f1, where='/tuscany', name=input.replace('.png', ''))
    image.save(fp=fnode, format='png')
    fnode.close()

f1.close()
```

```sh
/bin/rm -f small*png
ls -l storage.h5    # 2.2M - all our images are now stored inside this HDF5 file, along with the earlier text_file
ptdump storage.h5   # very useful command: dumps all objects from the file
```

Let's read and display these images:

```py
import tables as tt
from tables.nodes import filenode
from PIL import Image

f1 = tt.open_file('storage.h5', 'r')

image = Image.open(fp=f1.root.tuscany.small08)
image.show()

for i in f1.root.tuscany:   # cycle through all its children
    print(i)
 
f1.root.tuscany['small00']   # is another way to acces it

image = Image.open(fp=f1.root.tuscany['small08'])
image.show()
```

```py
import tables as tt
from tables.nodes import filenode
from PIL import Image

f1 = tt.open_file('storage.h5', 'a')
f1.root.tuscany.small19.remove()               # remove the node
f1.root.tuscany.small18.rename("last_image")   # rename the node

for i in f1.root.tuscany:   # cycle through all its children
    print(i)




```



Current limitations:
- node files are restricted in their naming (only valid Python identifiers are valid); use metadata to provide
  more description
- node files can only be opened for read-only or read+append mode; you can also rename and delete nodes
- only binary I/O is supported
- no universal newline support yet, besides `\n`






<!-- {{<a "link" "text">}} -->
